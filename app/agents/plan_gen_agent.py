from app.ai_initializer import get_ai_client
from app.config import config


def gen_plan(
    prompt: str,
    use_local_models: bool = False,
) -> str:
    """Функция для составления плана курса по промпту от llm

    Args:
        prompt (str): Промпт от llm
        use_local_models (bool): Использовать локальные модели или нет. Defaults to False

    Returns:
        str: План dict в виде str
    """
    json_example = """
    {
        "1": "Название пункта, которое отражает его наполнение",
        "1.1": "Название пункта, которое отражает его наполнение",
        "1.2": "Название пункта, которое отражает его наполнение",
        "2": "Название пункта, которое отражает его наполнение"
    }
    """
    system_prompt = f"""Ты профессиональный составитель планов для LLM. Тебе на вход передается промпт, по которому
    LLM должна составить курс, а твоя задача максимально правильно и рационально сделать план для нее же. Анализируй
    следующий пункт плана, опираясь на предыдущий. От тебя требуется только план и больше ничего: ни пояснений, ни
    ссылок на статьи, ни примеров проектов и тп. Не используй разметку md в своем ответе, ты пишешь для LLM. Пример
    твоего ответа: {json_example} (это пример!). У тебя также должны быть подпункты. Разделение на подпункты
    должно быть: 1. Логичным (Мажорный пункт для общей информации (1, 2, 3 и т.д.), Минорный(1.1, 1.2 и т.д.) -
    для углублений и уточнений); 2. Качественным. Также тебе стоит учесть то, что сложность должна нарастать в
    линейном виде, так как человек, которому ты составляешь план будет чаще всего учиться с нуля."""
    prompt_to_llm = f"""Промпт: {prompt}"""
    client = get_ai_client(use_local_models)
    result = client.message(
        model=(
            "mistral-large-latest"
            if not use_local_models
            else config.OLLAMA_MODEL_NAME
        ),
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": prompt_to_llm,
            },
        ],
        temperature=0.2,
        response_format={
            "type": "json_object",
        },
    )
    return result
