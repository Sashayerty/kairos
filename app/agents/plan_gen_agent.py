from app.ai_initializer import get_ai_client
from app.config import config


def gen_plan(
    prompt: str,
    use_local_models: bool = False,
) -> str:
    """Функция для составления плана курса по промпту от llm

    Args:
        prompt (str): Промпт от llm
        use_local_models (bool): Использовать локальные модели или нет. Defaults to False

    Returns:
        str: План dict в виде str
    """
    json_example = """
    {
        "1": "Название пункта, которое отражает его наполнение",
        "1.1": "Название пункта, которое отражает его наполнение",
        "1.2": "Название пункта, которое отражает его наполнение",
        "2": "Название пункта, которое отражает его наполнение"
    }
    """
    prompt_to_llm = f"""Привет! Ты профессиональный составитель планов для LLM. Тебе на вход передается промпт,
    по которому LLM должна составить курс, а твоя задача максимально правильно и рационально сделать план для нее
    же. Анализируй следующий пункт плана, опираясь на предыдущий. Промпт: {prompt}. От тебя требуется только план
    и больше ничего: ни пояснений, ни ссылок на статьи, ни примеров проектов и тп. Не используй разметку md в своем
    ответе, ты пишешь для LLM. Пример твоего ответа: {json_example}(это пример!). У тебя также должны быть
    подпункты. Разделение на подпункты должно быть: 1. Логичным (Мажорный пункт для общей информации (1, 2, 3 и т.д.),
    Минорный(1.1, 1.2 и т.д.) - для углублений и уточнений); 2. Качественным. Также тебе стоит учесть то, что
    сложность должна нарастать в линейном виде, так как человек, которому ты составляешь план будет чаще всего
    учиться с нуля."""
    client = get_ai_client(use_local_models)
    result = client.message(
        model=(
            "mistral-large-latest"
            if not use_local_models
            else config.OLLAMA_MODEL_NAME
        ),
        messages=[
            {
                "role": "user",
                "content": prompt_to_llm,
            }
        ],
        temperature=0.2,
        response_format={
            "type": "json_object",
        },
    )
    return result
